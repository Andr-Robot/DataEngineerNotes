**思路：** 如果一个样本在特征空间中的 `$k$` 个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。   
KNN的具体**步骤**：
1. 计算样本集中所有样本到达待测样本的欧式距离；
2. 选出离待测点最近的K个点，`$k$` 由用户指定；
3. 计算在这 `$k$` 个点中，各个类型的个数；
4. 将个数最多的类型作为待测样本的类型。   
 
**优点：**   
- 理论成熟，思想简单，既可以用来做分类也可以用来做回归；
- 可用于非线性分类；
- 训练时间复杂度为O(n)；
- 对数据没有假设，准确度高，对异常值不敏感。

**缺点：**   
- 计算量大（体现在距离计算上）；
- 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差；
- 需要大量内存。   

#### 参考文献
[[机器学习] ——KNN K-最邻近算法](https://www.cnblogs.com/yushuo1990/p/5879341.html)   